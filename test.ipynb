{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helper import draw_dot\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label='') -> None:\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        out = Value(self.data + other.data, _children=(self, other), _op='+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        out = Value(self.data * other.data, _children=(self, other), _op='*')\n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __neg__(self): \n",
    "        return -1.0 * self\n",
    "    \n",
    "    def exp(self): \n",
    "        out = Value(math.exp(self.data), _children=(self,), _op='exp')\n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.data\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other): # self ** other\n",
    "        if not (isinstance(other, int) or isinstance(other, float)):\n",
    "            assert False\n",
    "        out = Value(self.data ** other, _children=(self,), _op='**')\n",
    "        def _backward():\n",
    "            self.grad += out.grad * (other * (self.data ** (other - 1.0)))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + other.__neg__()\n",
    "\n",
    "    def __truediv__(self, other): # self/other\n",
    "        return self * (other ** (-1.0))\n",
    "    \n",
    "    # def tanh(self): \n",
    "    #     ex = math.exp(2*self.data)\n",
    "    #     out = Value((ex - 1)/(ex + 1), _children=(self,), _op='tanh')\n",
    "    #     def _backward(): \n",
    "    #         self.grad += (1 - out.data ** 2) * out.grad\n",
    "    #     out._backward = _backward\n",
    "    #     return out\n",
    "\n",
    "    def tanh(self):\n",
    "        e = (2*self).exp()\n",
    "        return (e - 1)/(e + 1)\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "            return\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# weights w1, w1\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "b = Value(6.88137, label='b')\n",
    "\n",
    "w1x1 = w1 * x1; w1x1.label = \"w1*x1\"\n",
    "w2x2 = w2 * x2; w2x2.label = \"w2*x2\"\n",
    "w1x1w2x2 = w1x1 + w2x2; w1x1w2x2.label = \"w1*x1 + w2*x2\"\n",
    "z = w1x1w2x2 + b; z.label = \"z\"\n",
    "# a = z.tanh(); a.label = \"a\"\n",
    "\n",
    "e = (2*z).exp()\n",
    "a = (e - 1)/(e + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=-0.668937786504842)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.weights = [Value(random.uniform(-1.0, 1.0)) for i in range(nin)]\n",
    "        self.bias = Value(random.uniform(-1.0, 1.0))\n",
    "\n",
    "    def __call__(self, x): # forward pass\n",
    "        _sum = self.bias\n",
    "        for xi, wi in zip(x, self.weights):\n",
    "            _sum = _sum + (xi * wi)\n",
    "        return _sum.tanh()\n",
    "\n",
    "class Layer: \n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for i in range(nout)]\n",
    "\n",
    "    def __call__(self, x): # x is nin\n",
    "        out = [neuron(x) for neuron in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "class MLP: \n",
    "    def __init__(self, nin, nouts):\n",
    "        self.layers = []\n",
    "        prev = nin\n",
    "        for i in nouts:\n",
    "            self.layers.append(Layer(prev, i))\n",
    "            prev = i\n",
    "\n",
    "    def __call__(self, x): # forward pass\n",
    "        y = x\n",
    "        for i in self.layers:\n",
    "            y = i(y)\n",
    "        return y\n",
    "            \n",
    "\n",
    "l = MLP(3, [4, 4, 1])\n",
    "# xs = [\n",
    "#     [2, 3, 1],\n",
    "#     [1, -6, 2],\n",
    "#     [0, -1, 4],\n",
    "#     [0, -1, 1]\n",
    "# ]\n",
    "x = [1, 3, 4]\n",
    "print(l(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.6157934932427658),\n",
       " Value(data=-0.6695012453374227),\n",
       " Value(data=-0.7335987738673829),\n",
       " Value(data=0.802195979312708)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "ypred = [l(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36694011055313147\n",
      "0.344946493714067\n",
      "0.3245814696678245\n",
      "0.30571039072160955\n",
      "0.28821217947423544\n",
      "0.2719775741074356\n",
      "0.25690760804934887\n",
      "0.2429123031735468\n",
      "0.2299095542926688\n",
      "0.2178241826401788\n",
      "0.2065871367851377\n",
      "0.19613482063832632\n",
      "0.18640852966459223\n",
      "0.17735397799825375\n",
      "0.16892090081305114\n",
      "0.1610627180042923\n",
      "0.15373624698324959\n",
      "0.14690145413828878\n",
      "0.1405212362460774\n",
      "0.13456122477211058\n",
      "0.1289896075332932\n",
      "0.1237769635633299\n",
      "0.11889610819386352\n",
      "0.11432194632619117\n",
      "0.11003133262134218\n",
      "0.10600293789490232\n",
      "0.1022171213911732\n",
      "0.09865580885813774\n",
      "0.09530237648068271\n",
      "0.09214154078348664\n",
      "0.08915925461234699\n",
      "0.08634260926454894\n",
      "0.08367974278155316\n",
      "0.0811597543528869\n",
      "0.07877262471693351\n",
      "0.07650914238754819\n",
      "0.0743608354879423\n",
      "0.07231990893620974\n",
      "0.07037918670019418\n",
      "0.068532058822346\n",
      "0.06677243290662369\n",
      "0.06509468975800206\n",
      "0.06349364286939226\n",
      "0.061964501459464805\n",
      "0.06050283677683687\n",
      "0.059104551400361804\n",
      "0.05776585128098388\n",
      "0.05648322028715216\n",
      "0.05525339703257326\n",
      "0.05407335378172827\n",
      "0.052940277244777484\n",
      "0.051851551089020696\n",
      "0.05080474000882192\n",
      "0.0497975752097558\n",
      "0.048827941175654095\n",
      "0.04789386359919531\n",
      "0.04699349836771956\n",
      "0.04612512150607898\n",
      "0.04528711998759293\n",
      "0.044477983332632115\n",
      "0.04369629592203042\n",
      "0.042940729959502696\n",
      "0.04221003902355986\n",
      "0.04150305215513402\n",
      "0.0408186684322914\n",
      "0.040155851988075225\n",
      "0.039513627431734526\n",
      "0.03889107563738817\n",
      "0.03828732986760083\n",
      "0.03770157220242993\n",
      "0.037133030247286286\n",
      "0.03658097409545744\n",
      "0.036044713523402216\n",
      "0.03552359539896105\n",
      "0.035017001284465846\n",
      "0.03452434521838829\n",
      "0.03404507166066042\n",
      "0.03357865358815393\n",
      "0.03312459072801962\n",
      "0.032682407917693744\n",
      "0.032251653581370374\n",
      "0.03183189831364376\n",
      "0.031422733561834726\n",
      "0.03102377039925919\n",
      "0.030634638382360423\n",
      "0.030254984485238515\n",
      "0.029884472105656264\n",
      "0.02952278013710308\n",
      "0.029169602101952987\n",
      "0.028824645341163288\n",
      "0.02848763025633707\n",
      "0.028158289600315575\n",
      "0.027836367812775768\n",
      "0.02752162039759453\n",
      "0.027213813338996473\n",
      "0.02691272255374415\n",
      "0.02661813337683865\n",
      "0.02632984007840152\n",
      "0.026047645409586356\n",
      "0.025771360175534587\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    ypred = [l(x) for x in xs]\n",
    "\n",
    "    loss = Value(0.0)\n",
    "    for y, yp in zip(ys, ypred):\n",
    "        loss = loss + (yp - y) ** 2\n",
    "\n",
    "    print(loss.data)\n",
    "    loss.backward()\n",
    "\n",
    "    alpha = 0.01\n",
    "\n",
    "    for i, layer in enumerate(l.layers):\n",
    "        for j, neuron in enumerate(layer.neurons):\n",
    "            for k, weight in enumerate(neuron.weights):\n",
    "                l.layers[i].neurons[j].weights[k] = weight - alpha * weight.grad;\n",
    "            l.layers[i].neurons[j].bias = neuron.bias - alpha * neuron.bias.grad;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [1]\n",
    "for i in test:\n",
    "    i = 3\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, -1.0, -1.0, 1.0]\n",
      "[Value(data=0.9259784734387891), Value(data=-0.9023769948141576), Value(data=-0.912883019867243), Value(data=0.9436745669158564)]\n"
     ]
    }
   ],
   "source": [
    "print(ys)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
