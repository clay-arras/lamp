First:

- need to make the tests better
- make the zero_grad() function and module
- need to make the tanh backprop better
- need to make the NN more efficient
- leaf flag on which gradients to calculate

Extensions:

<!-- - figure out how to do MNIST in numpy -->
<!-- - can try to make this work with MNIST -->

- can make this work better with vectors and batches
- create some basic numpy function like numpy dot product
- can try to write some rudiementary CUDA code

https://github.com/vincentlaucsb/csv-parser

Next Steps:

- figure out how to CPU-bound some of the for loops
- train a small network on MNIST short
- finally, try to apply learnings to C++ code
- finally, try to add vector parallization

Without Optimizations: 1m 9.5s
