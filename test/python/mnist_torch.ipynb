{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Neural Network from Scratch (PyTorch)\n",
    "\n",
    "Simple implementation of a neural network from scratch using PyTorch's autograd for MNIST digit classification. Built as a proof of concept before implementing it in C++. The accuracy is around 92% for validation, if you make the network deeper you can probably get it higher.\n",
    "\n",
    "## Architecture\n",
    "- Input (784) → Hidden (256) → Output (10)\n",
    "- ReLU + Softmax activations\n",
    "- Cross-entropy loss\n",
    "\n",
    "## Setup\n",
    "1. Place MNIST CSV files in `data/`\n",
    "2. Update PROJECT_ROOT path\n",
    "3. Run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "PROJECT_ROOT = \"/home/nlin/workspace/code/projects/autograd_cpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_from_file(csvfile):\n",
    "    reader = csv.reader(csvfile)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        row = [float(i) for i in row]\n",
    "        data.append([i * 1.0 / 255 for i in row[1:]])\n",
    "        labels.append([1 if row[0] == i else 0 for i in range(10)])\n",
    "\n",
    "    n = len(data)\n",
    "    xs = torch.tensor(data)\n",
    "    ys = torch.tensor(labels)\n",
    "    csvfile.close()\n",
    "\n",
    "    return xs, ys, n\n",
    "\n",
    "\n",
    "csvfile = open(os.path.join(PROJECT_ROOT, \"data/mnist_train.csv\"))\n",
    "xs, ys, n = load_mnist_from_file(csvfile)\n",
    "xs = xs.cuda()\n",
    "ys = ys.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE = False\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if VISUALIZE:\n",
    "    plt.figure(figsize=(28, 28))\n",
    "    for idx, i in enumerate(xs[:5]):\n",
    "        image = torch.reshape(i, (28, 28))\n",
    "        plt.subplot(1, 5, idx + 1)\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    eX = torch.exp(X)\n",
    "    sum_eX = torch.sum(eX, dim=1)\n",
    "    t_sum_eX = sum_eX[None]\n",
    "    pt_sum_eX = torch.permute(t_sum_eX, (1, 0))\n",
    "    return eX / pt_sum_eX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "\n",
    "W1 = (torch.rand((784, 256), requires_grad=True) - 0.5).cuda()\n",
    "W2 = (torch.rand((256, 10), requires_grad=True) - 0.5).cuda()\n",
    "W1 = W1.detach().requires_grad_()\n",
    "W2 = W2.detach().requires_grad_()\n",
    "assert W1.is_leaf\n",
    "assert W2.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcsvfile = open(os.path.join(PROJECT_ROOT, \"data/mnist_test_short.csv\"))\n",
    "txs, tys, tn = load_mnist_from_file(tcsvfile)\n",
    "txs = txs.cuda()\n",
    "tys = tys.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0; Loss: 3.5505425930023193, Accuracy: 0.4710833430290222\n",
      "Validation Accuracy:  0.4490000009536743\n",
      "Model Saved\n",
      "Iteration 1000; Loss: 1.0749568939208984, Accuracy: 0.7512999773025513\n",
      "Iteration 2000; Loss: 0.9173344969749451, Accuracy: 0.7866166830062866\n",
      "Iteration 3000; Loss: 0.8260608911514282, Accuracy: 0.8073499798774719\n",
      "Iteration 4000; Loss: 0.7628840208053589, Accuracy: 0.8224833607673645\n",
      "Iteration 5000; Loss: 0.7152251601219177, Accuracy: 0.8338000178337097\n",
      "Iteration 6000; Loss: 0.6773340702056885, Accuracy: 0.8420500159263611\n",
      "Iteration 7000; Loss: 0.6460137963294983, Accuracy: 0.8483999967575073\n",
      "Iteration 8000; Loss: 0.6193917989730835, Accuracy: 0.8546333312988281\n",
      "Iteration 9000; Loss: 0.5963143706321716, Accuracy: 0.8592833280563354\n",
      "Iteration 10000; Loss: 0.5759768486022949, Accuracy: 0.8637999892234802\n",
      "Validation Accuracy:  0.8569999933242798\n",
      "Iteration 11000; Loss: 0.5578020811080933, Accuracy: 0.8676499724388123\n",
      "Iteration 12000; Loss: 0.541381299495697, Accuracy: 0.8711666464805603\n",
      "Iteration 13000; Loss: 0.5264291167259216, Accuracy: 0.8744166493415833\n",
      "Iteration 14000; Loss: 0.5127310156822205, Accuracy: 0.8771166801452637\n",
      "Iteration 15000; Loss: 0.50009685754776, Accuracy: 0.8792833089828491\n",
      "Iteration 16000; Loss: 0.4883826971054077, Accuracy: 0.8820499777793884\n",
      "Iteration 17000; Loss: 0.4774707853794098, Accuracy: 0.8842333555221558\n",
      "Iteration 18000; Loss: 0.46726009249687195, Accuracy: 0.8865000009536743\n",
      "Iteration 19000; Loss: 0.4576740264892578, Accuracy: 0.8883000016212463\n",
      "Iteration 20000; Loss: 0.4486537277698517, Accuracy: 0.8900499939918518\n",
      "Validation Accuracy:  0.8769999742507935\n",
      "Iteration 21000; Loss: 0.4401428997516632, Accuracy: 0.89205002784729\n",
      "Iteration 22000; Loss: 0.4320860803127289, Accuracy: 0.893750011920929\n",
      "Iteration 23000; Loss: 0.4244483411312103, Accuracy: 0.8951833248138428\n",
      "Iteration 24000; Loss: 0.41719329357147217, Accuracy: 0.8967666625976562\n",
      "Iteration 25000; Loss: 0.41028717160224915, Accuracy: 0.8979833126068115\n",
      "Iteration 26000; Loss: 0.403697669506073, Accuracy: 0.899566650390625\n",
      "Iteration 27000; Loss: 0.39740538597106934, Accuracy: 0.9007499814033508\n",
      "Iteration 28000; Loss: 0.3913796544075012, Accuracy: 0.9017333388328552\n",
      "Iteration 29000; Loss: 0.38560807704925537, Accuracy: 0.9028833508491516\n",
      "Iteration 30000; Loss: 0.38007792830467224, Accuracy: 0.9039499759674072\n",
      "Validation Accuracy:  0.8899999856948853\n",
      "Iteration 31000; Loss: 0.3747677803039551, Accuracy: 0.9050999879837036\n",
      "Iteration 32000; Loss: 0.3696637749671936, Accuracy: 0.906083345413208\n",
      "Iteration 33000; Loss: 0.3647468686103821, Accuracy: 0.9074666500091553\n",
      "Iteration 34000; Loss: 0.3600079417228699, Accuracy: 0.908466637134552\n",
      "Iteration 35000; Loss: 0.35543519258499146, Accuracy: 0.9093666672706604\n",
      "Iteration 36000; Loss: 0.35101836919784546, Accuracy: 0.9103999733924866\n",
      "Iteration 37000; Loss: 0.3467501401901245, Accuracy: 0.911383330821991\n",
      "Iteration 38000; Loss: 0.3426210284233093, Accuracy: 0.9121500253677368\n",
      "Iteration 39000; Loss: 0.3386222720146179, Accuracy: 0.9130499958992004\n",
      "Iteration 40000; Loss: 0.3347492516040802, Accuracy: 0.9139333367347717\n",
      "Validation Accuracy:  0.902999997138977\n",
      "Iteration 41000; Loss: 0.3309929072856903, Accuracy: 0.9146833419799805\n",
      "Iteration 42000; Loss: 0.3273462951183319, Accuracy: 0.9153500199317932\n",
      "Iteration 43000; Loss: 0.32380956411361694, Accuracy: 0.9160166382789612\n",
      "Iteration 44000; Loss: 0.3203749358654022, Accuracy: 0.9166333079338074\n",
      "Iteration 45000; Loss: 0.31703686714172363, Accuracy: 0.9175000190734863\n",
      "Iteration 46000; Loss: 0.31379616260528564, Accuracy: 0.918316662311554\n",
      "Iteration 47000; Loss: 0.31064334511756897, Accuracy: 0.9191333055496216\n",
      "Iteration 48000; Loss: 0.3075757920742035, Accuracy: 0.9196500182151794\n",
      "Iteration 49000; Loss: 0.30458876490592957, Accuracy: 0.9204833507537842\n",
      "Iteration 50000; Loss: 0.3016798198223114, Accuracy: 0.9211000204086304\n",
      "Validation Accuracy:  0.9049999713897705\n",
      "Iteration 51000; Loss: 0.29884639382362366, Accuracy: 0.9217166900634766\n",
      "Iteration 52000; Loss: 0.2960851490497589, Accuracy: 0.9223999977111816\n",
      "Iteration 53000; Loss: 0.29339128732681274, Accuracy: 0.9229166507720947\n",
      "Iteration 54000; Loss: 0.29076141119003296, Accuracy: 0.9232333302497864\n",
      "Iteration 55000; Loss: 0.2881944179534912, Accuracy: 0.9237666726112366\n",
      "Iteration 56000; Loss: 0.28568825125694275, Accuracy: 0.9243333339691162\n",
      "Iteration 57000; Loss: 0.2832414209842682, Accuracy: 0.9250166416168213\n",
      "Iteration 58000; Loss: 0.280849814414978, Accuracy: 0.9254000186920166\n",
      "Iteration 59000; Loss: 0.2785111367702484, Accuracy: 0.9259333610534668\n",
      "Iteration 60000; Loss: 0.2762259542942047, Accuracy: 0.9263833165168762\n",
      "Validation Accuracy:  0.9089999794960022\n",
      "Iteration 61000; Loss: 0.2739904224872589, Accuracy: 0.9269999861717224\n",
      "Iteration 62000; Loss: 0.27180302143096924, Accuracy: 0.9276000261306763\n",
      "Iteration 63000; Loss: 0.26966437697410583, Accuracy: 0.9279333353042603\n",
      "Iteration 64000; Loss: 0.26756852865219116, Accuracy: 0.9283000230789185\n",
      "Iteration 65000; Loss: 0.2655170261859894, Accuracy: 0.9286999702453613\n",
      "Iteration 66000; Loss: 0.2635077238082886, Accuracy: 0.9291666746139526\n",
      "Iteration 67000; Loss: 0.2615400552749634, Accuracy: 0.9295333623886108\n",
      "Iteration 68000; Loss: 0.25960996747016907, Accuracy: 0.9298999905586243\n",
      "Iteration 69000; Loss: 0.2577156722545624, Accuracy: 0.9301833510398865\n",
      "Iteration 70000; Loss: 0.2558579742908478, Accuracy: 0.9305166602134705\n",
      "Validation Accuracy:  0.9150000214576721\n",
      "Iteration 71000; Loss: 0.2540355622768402, Accuracy: 0.9310666918754578\n",
      "Iteration 72000; Loss: 0.25224900245666504, Accuracy: 0.9315833449363708\n",
      "Iteration 73000; Loss: 0.25049564242362976, Accuracy: 0.9320666790008545\n",
      "Iteration 74000; Loss: 0.24877223372459412, Accuracy: 0.9324166774749756\n",
      "Iteration 75000; Loss: 0.24708040058612823, Accuracy: 0.9329500198364258\n",
      "Iteration 76000; Loss: 0.24541936814785004, Accuracy: 0.9332166910171509\n",
      "Iteration 77000; Loss: 0.24378825724124908, Accuracy: 0.9336833357810974\n",
      "Iteration 78000; Loss: 0.24218635261058807, Accuracy: 0.9340333342552185\n",
      "Iteration 79000; Loss: 0.2406124472618103, Accuracy: 0.9345499873161316\n",
      "Iteration 80000; Loss: 0.23906487226486206, Accuracy: 0.9350666403770447\n",
      "Validation Accuracy:  0.9169999957084656\n",
      "Iteration 81000; Loss: 0.23754316568374634, Accuracy: 0.935533344745636\n",
      "Iteration 82000; Loss: 0.2360459715127945, Accuracy: 0.9359833598136902\n",
      "Iteration 83000; Loss: 0.23457454144954681, Accuracy: 0.9362499713897705\n",
      "Iteration 84000; Loss: 0.23312769830226898, Accuracy: 0.9366166591644287\n",
      "Iteration 85000; Loss: 0.23170508444309235, Accuracy: 0.9369333386421204\n",
      "Iteration 86000; Loss: 0.23030424118041992, Accuracy: 0.9372166395187378\n",
      "Iteration 87000; Loss: 0.22892668843269348, Accuracy: 0.937583327293396\n",
      "Iteration 88000; Loss: 0.2275710254907608, Accuracy: 0.9377999901771545\n",
      "Iteration 89000; Loss: 0.2262362688779831, Accuracy: 0.9381666779518127\n",
      "Iteration 90000; Loss: 0.22492142021656036, Accuracy: 0.9384833574295044\n",
      "Validation Accuracy:  0.9210000038146973\n",
      "Iteration 91000; Loss: 0.22362613677978516, Accuracy: 0.9388333559036255\n",
      "Iteration 92000; Loss: 0.22235123813152313, Accuracy: 0.9390166401863098\n",
      "Iteration 93000; Loss: 0.2210954874753952, Accuracy: 0.939383327960968\n",
      "Iteration 94000; Loss: 0.2198573499917984, Accuracy: 0.9396499991416931\n",
      "Iteration 95000; Loss: 0.21863649785518646, Accuracy: 0.9399333596229553\n",
      "Iteration 96000; Loss: 0.21743401885032654, Accuracy: 0.9401833415031433\n",
      "Iteration 97000; Loss: 0.21624861657619476, Accuracy: 0.940666675567627\n",
      "Iteration 98000; Loss: 0.2150806039571762, Accuracy: 0.940933346748352\n",
      "Iteration 99000; Loss: 0.2139289379119873, Accuracy: 0.9412999749183655\n"
     ]
    }
   ],
   "source": [
    "NUM_ITER = 100000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for it in range(NUM_ITER):\n",
    "    A1 = torch.mm(xs, W1)\n",
    "    Z1 = torch.where(A1 > 0, A1, 0)\n",
    "    A2 = torch.mm(Z1, W2)\n",
    "    Z2 = softmax(A2)\n",
    "\n",
    "    loss = -1 / n * torch.sum(torch.log(Z2) * ys)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        W1.grad.zero_()\n",
    "        W2.grad.zero_()\n",
    "\n",
    "        assert W1.is_leaf\n",
    "        assert W2.is_leaf\n",
    "\n",
    "    y_pred = torch.argmax(Z2, dim=1)\n",
    "    y_true = torch.argmax(ys, dim=1)\n",
    "    if it % 1000 == 0:\n",
    "        print(\n",
    "            f\"Iteration {it}; Loss: {loss.cpu().float()}, Accuracy: {(torch.sum(y_pred.cpu() == y_true.cpu()) / n).float()}\"\n",
    "        )\n",
    "\n",
    "    if it % 10000 == 0:\n",
    "        t_A1 = torch.mm(txs, W1)\n",
    "        t_Z1 = torch.where(t_A1 > 0, t_A1, 0)\n",
    "        t_A2 = torch.mm(t_Z1, W2)\n",
    "        t_Z2 = softmax(t_A2)\n",
    "\n",
    "        t_y_pred = torch.argmax(t_Z2, dim=1)\n",
    "        t_y_true = torch.argmax(tys, dim=1)\n",
    "        print(\n",
    "            \"Validation Accuracy: \",\n",
    "            (torch.sum(t_y_pred.cpu() == t_y_true.cpu()) / tn).float().item(),\n",
    "        )\n",
    "\n",
    "    if it % 100000 == 0:\n",
    "        torch.save(\n",
    "            (W1.cpu(), W2.cpu()),\n",
    "            os.path.join(PROJECT_ROOT, f\"checkpoint/mnist_torch_ckpt_{it}.pt\"),\n",
    "        )\n",
    "        print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    (W1.cpu(), W2.cpu()),\n",
    "    os.path.join(PROJECT_ROOT, f\"checkpoint/mnist_torch_ckpt_final.pt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.9229999780654907\n"
     ]
    }
   ],
   "source": [
    "ltcsvfile = open(os.path.join(PROJECT_ROOT, \"data/mnist_test.csv\"))\n",
    "ltxs, ltys, ltn = load_mnist_from_file(ltcsvfile)\n",
    "ltxs = ltxs.cuda()\n",
    "ltys = ltys.cuda()\n",
    "\n",
    "lt_A1 = torch.mm(txs, W1)\n",
    "lt_Z1 = torch.where(lt_A1 > 0, lt_A1, 0)\n",
    "lt_A2 = torch.mm(lt_Z1, W2)\n",
    "lt_Z2 = softmax(lt_A2)\n",
    "\n",
    "lt_y_pred = torch.argmax(lt_Z2, dim=1)\n",
    "lt_y_true = torch.argmax(tys, dim=1)\n",
    "print(\n",
    "    \"Validation Accuracy: \",\n",
    "    (torch.sum(lt_y_pred.cpu() == lt_y_true.cpu()) / tn).float().item(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
