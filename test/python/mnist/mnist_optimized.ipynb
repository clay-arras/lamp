{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Neural Network with NumPy (Optimized)\n",
    "\n",
    "Simple implementation of a neural network using NumPy for MNIST digit classification. This version achieves ~96% validation accuracy through optimizations including batch gradient descent and efficient matrix operations. Uses Jacobian Vector products for a very big speed boost, should be pretty much the best we can do without GPU.\n",
    "\n",
    "## Architecture\n",
    "- Input (784) → Hidden (256) → Output (10)\n",
    "- ReLU + Softmax activations\n",
    "- Cross-entropy loss\n",
    "- Batch gradient descent with size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "PROJECT_ROOT = \"/home/nlin/workspace/code/projects/autograd_cpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_from_file(csvfile):\n",
    "    reader = csv.reader(csvfile)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        row = [float(i) for i in row]\n",
    "        data.append([i * 1.0 / 255 for i in row[1:]])\n",
    "        labels.append([1 if row[0] == i else 0 for i in range(10)])\n",
    "\n",
    "    n = len(data)\n",
    "    xs = np.array(data)\n",
    "    ys = np.array(labels)\n",
    "    csvfile.close()\n",
    "\n",
    "    return xs, ys, n\n",
    "\n",
    "\n",
    "csvfile = open(os.path.join(PROJECT_ROOT, \"data/mnist_train.csv\"))\n",
    "xs, ys, n = load_mnist_from_file(csvfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    max_X = np.max(X, axis=1, keepdims=True)\n",
    "    eX = np.exp(X - max_X)\n",
    "    sum_eX = np.sum(eX, axis=1, keepdims=True)\n",
    "    return eX / sum_eX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.rand(784, 256) - 0.5\n",
    "W2 = np.random.rand(256, 10) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcsvfile = open(os.path.join(PROJECT_ROOT, \"data/mnist_test_short.csv\"))\n",
    "txs, tys, tn = load_mnist_from_file(tcsvfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_ITER = 100000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for it in range(NUM_ITER):\n",
    "    batch_indices = np.random.choice(n, BATCH_SIZE, replace=False)\n",
    "    batch_xs = xs[batch_indices]\n",
    "    batch_ys = ys[batch_indices]\n",
    "\n",
    "    A1 = np.matmul(batch_xs, W1)\n",
    "    Z1 = np.where(A1 > 0, A1, 0)\n",
    "    A2 = np.matmul(Z1, W2)\n",
    "    Z2 = softmax(A2)\n",
    "\n",
    "    loss = -np.sum(np.log(Z2 + 1e-10) * batch_ys)\n",
    "\n",
    "    dLda2 = Z2 - batch_ys\n",
    "    dLdw2 = np.matmul(Z1.T, dLda2)\n",
    "    dLdz1 = np.matmul(dLda2, W2.T)\n",
    "    dLda1 = np.multiply(dLdz1, np.where(A1 > 0, 1, 0))\n",
    "    dLdw1 = np.matmul(batch_xs.T, dLda1)\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    W1 -= learning_rate * dLdw1\n",
    "    W2 -= learning_rate * dLdw2\n",
    "\n",
    "    y_pred = np.argmax(Z2, axis=1)\n",
    "    y_true = np.argmax(batch_ys, axis=1)\n",
    "    if it % 1000 == 0:\n",
    "        print(\n",
    "            f\"Iteration {it}; Loss: {float(loss)}, Accuracy: {np.sum(y_pred == y_true) / BATCH_SIZE}\"\n",
    "        )\n",
    "\n",
    "    if it % 10000 == 0:\n",
    "        t_A1 = np.matmul(txs, W1)\n",
    "        t_Z1 = np.where(t_A1 > 0, t_A1, 0)\n",
    "        t_A2 = np.matmul(t_Z1, W2)\n",
    "        t_Z2 = softmax(t_A2)\n",
    "\n",
    "        t_y_pred = np.argmax(t_Z2, axis=1)\n",
    "        t_y_true = np.argmax(tys, axis=1)\n",
    "        print(\"Validation Accuracy: \", np.sum(t_y_pred == t_y_true) / tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
