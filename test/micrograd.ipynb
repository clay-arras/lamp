{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from Karpathy's micrograd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helper import draw_dot\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label='') -> None:\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        out = Value(self.data + other.data, _children=(self, other), _op='+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        out = Value(self.data * other.data, _children=(self, other), _op='*')\n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __neg__(self): \n",
    "        return -1.0 * self\n",
    "    \n",
    "    def exp(self): \n",
    "        out = Value(math.exp(self.data), _children=(self,), _op='exp')\n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.data\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other): # self ** other\n",
    "        if not (isinstance(other, int) or isinstance(other, float)):\n",
    "            assert False\n",
    "        out = Value(self.data ** other, _children=(self,), _op='**')\n",
    "        def _backward():\n",
    "            self.grad += out.grad * (other * (self.data ** (other - 1.0)))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + other.__neg__()\n",
    "\n",
    "    def __truediv__(self, other): # self/other\n",
    "        return self * (other ** (-1.0))\n",
    "\n",
    "    def tanh(self):\n",
    "        e = (2*self).exp()\n",
    "        return (e - 1)/(e + 1)\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "            return\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# weights w1, w1\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "b = Value(6.88137, label='b')\n",
    "\n",
    "w1x1 = w1 * x1; w1x1.label = \"w1*x1\"\n",
    "w2x2 = w2 * x2; w2x2.label = \"w2*x2\"\n",
    "w1x1w2x2 = w1x1 + w2x2; w1x1w2x2.label = \"w1*x1 + w2*x2\"\n",
    "z = w1x1w2x2 + b; z.label = \"z\"\n",
    "# a = z.tanh(); a.label = \"a\"\n",
    "\n",
    "e = (2*z).exp()\n",
    "a = (e - 1)/(e + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.weights = [Value(random.uniform(-1.0, 1.0)) for i in range(nin)]\n",
    "        self.bias = Value(random.uniform(-1.0, 1.0))\n",
    "\n",
    "    def __call__(self, x): # forward pass\n",
    "        _sum = self.bias\n",
    "        for xi, wi in zip(x, self.weights):\n",
    "            _sum = _sum + (xi * wi)\n",
    "        return _sum.tanh()\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.weights + [self.bias]\n",
    "\n",
    "class Layer: \n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for i in range(nout)]\n",
    "\n",
    "    def __call__(self, x): # x is nin\n",
    "        out = [neuron(x) for neuron in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            ps = neuron.parameters()\n",
    "            params.extend(ps)\n",
    "        return params\n",
    "        # return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP: \n",
    "    def __init__(self, nin, nouts):\n",
    "        self.layers = []\n",
    "        prev = nin\n",
    "        for i in nouts:\n",
    "            self.layers.append(Layer(prev, i))\n",
    "            prev = i\n",
    "\n",
    "    def __call__(self, x): # forward pass\n",
    "        y = x\n",
    "        for i in self.layers:\n",
    "            y = i(y)\n",
    "        return y\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            ps = layer.parameters()\n",
    "            params.extend(ps)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.7314358228821312),\n",
       " Value(data=0.7999750490721166),\n",
       " Value(data=0.8583563900091744),\n",
       " Value(data=0.2434636526515217)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP(3, [4, 4, 1])\n",
    "            \n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "ypred = [net(x) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    ypred = [net(x) for x in xs]\n",
    "\n",
    "    loss = Value(0.0)\n",
    "    for y, yp in zip(ys, ypred):\n",
    "        loss = loss + (yp - y) ** 2\n",
    "\n",
    "    # print(loss.data)\n",
    "    loss.backward()\n",
    "    for value in net.parameters():\n",
    "        value.data -= alpha * value.grad\n",
    "        value.grad = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
