{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, random\n",
    "sys.path.append(\"./build\")\n",
    "from cpp_custom_bind import *\n",
    "import torch\n",
    "\n",
    "EPS = 1e-4\n",
    "_LO_N, _HI_N = -5, 5\n",
    "_LO_R, _HI_R = 4, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_matrix(rows, cols, lo=_LO_N, hi=_HI_N):\n",
    "    return [[random.uniform(lo, hi) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def rand_shape(min_r=_LO_R, max_r=_HI_R, min_c=_LO_R, max_c=_HI_R):\n",
    "    return random.randint(min_r, max_r), random.randint(min_c, max_c)\n",
    "\n",
    "def is_close(a, b, eps=EPS):\n",
    "    return torch.all(torch.abs(torch.tensor(a) - torch.tensor(b)) < eps)\n",
    "\n",
    "def sample_unary():\n",
    "    r, c = rand_shape()\n",
    "    return [rand_matrix(r, c)]\n",
    "\n",
    "def sample_binary_same():\n",
    "    r, c = rand_shape()\n",
    "    return [rand_matrix(r, c), rand_matrix(r, c)]\n",
    "\n",
    "def sample_matmul():\n",
    "    m = random.randint(4, 8)\n",
    "    k = random.randint(4, 8)\n",
    "    n = random.randint(4, 8)\n",
    "    return [rand_matrix(m, k), rand_matrix(k, n)]\n",
    "\n",
    "def _to_col_major(mat):\n",
    "    return torch.tensor(mat).T.flatten().tolist()\n",
    "\n",
    "def _from_col_major(flat, like):\n",
    "    t = torch.tensor(flat).reshape(torch.tensor(like).T.shape).T\n",
    "    return t.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cpp_var(mat, requires_grad=True):\n",
    "    ten = cTensor(_to_col_major(mat), list(torch.tensor(mat).shape))\n",
    "    return cVariable(ten, requires_grad)\n",
    "\n",
    "def compute_grads(cpp_op, torch_op, mats, *extra):\n",
    "    torch_vars = [torch.tensor(m, dtype=torch.float64, requires_grad=True) for m in mats]\n",
    "    torch_out = torch_op(*torch_vars, *extra)\n",
    "    torch_out.backward(torch.ones_like(torch_out, dtype=torch.float64))\n",
    "    torch_grads = [v.grad.tolist() for v in torch_vars]\n",
    "    cpp_vars = [make_cpp_var(m) for m in mats]\n",
    "    cpp_op(*cpp_vars, *extra)\n",
    "    cpp_grads = [_from_col_major(v.grad.data, m) for v, m in zip(cpp_vars, mats)]\n",
    "    return cpp_grads, torch_grads\n",
    "\n",
    "def run_test(name, cpp_op, torch_op, sampler, *extra):\n",
    "    mats = sampler()\n",
    "    cpp_grads, torch_grads = compute_grads(cpp_op, torch_op, mats, *extra)\n",
    "    for cg, tg in zip(cpp_grads, torch_grads):\n",
    "        if not is_close(cg, tg):\n",
    "            print(cg)\n",
    "            print(tg)\n",
    "        assert is_close(cg, tg), f\"{name} failed, {torch.max(torch.abs(torch.tensor(cg) - torch.tensor(tg)))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TEST_CASES = [\n",
    "    (\"add\", add, torch.add, sample_binary_same),\n",
    "    (\"sub\", sub, torch.sub, sample_binary_same),\n",
    "    (\"mul\", mul, torch.mul, sample_binary_same),\n",
    "    (\"div\", div, torch.div, sample_binary_same),\n",
    "    (\"relu\", relu, torch.relu, sample_unary),\n",
    "    (\"exp\", exp, torch.exp, sample_unary),\n",
    "    (\"log\", log, torch.log, sample_unary),\n",
    "    (\"matmul\", matmul, torch.matmul, sample_matmul),\n",
    "    (\"transpose\", transpose, lambda x: x.T, sample_unary),\n",
    "    (\"sum_axis0\", lambda x, axis: sum(x, axis), lambda t, axis: torch.sum(t, dim=axis), sample_unary, 0),\n",
    "    (\"sum_axis1\", lambda x, axis: sum(x, axis), lambda t, axis: torch.sum(t, dim=axis), sample_unary, 1),\n",
    "]\n",
    "\n",
    "for case in TEST_CASES:\n",
    "    name, cpp_op, torch_op, sampler, *extra = case\n",
    "    run_test(name, cpp_op, torch_op, sampler, *extra)\n",
    "\n",
    "print(\"All gradient checks passed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
