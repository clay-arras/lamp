{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.3086204528808594\n",
      "Epoch 2, Loss: 2.300020217895508\n",
      "Epoch 3, Loss: 2.300004720687866\n",
      "Epoch 4, Loss: 2.300002336502075\n",
      "Epoch 5, Loss: 2.3000001907348633\n",
      "Epoch 6, Loss: 2.2999982833862305\n",
      "Epoch 7, Loss: 2.2999958992004395\n",
      "Epoch 8, Loss: 2.2999939918518066\n",
      "Epoch 9, Loss: 2.2999916076660156\n",
      "Epoch 10, Loss: 2.299989938735962\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- Helper functions to simulate CSV reading ---\n",
    "def read_csv(file_path):\n",
    "    # For demonstration, we assume MNIST dummy data:\n",
    "    num_examples = 1200\n",
    "    num_features = 784\n",
    "    num_classes = 10\n",
    "\n",
    "    # Create dummy input data (row-major order for PyTorch):\n",
    "    # In Eigen, the data might be stored as (784,1200) and then transposed.\n",
    "    # Here we directly use shape (1200,784)\n",
    "    data = np.random.rand(num_examples, num_features).astype(np.float32)\n",
    "    \n",
    "    # Create dummy one-hot labels with shape (1200,10)\n",
    "    labels = np.zeros((num_examples, num_classes), dtype=np.float32)\n",
    "    random_classes = np.random.randint(0, num_classes, size=num_examples)\n",
    "    labels[np.arange(num_examples), random_classes] = 1.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# --- Read data ---\n",
    "data_np, labels_np = read_csv(\"data/mnist_dummy.csv\")\n",
    "# In PyTorch, inputs have shape (1200, 784) and labels (1200, 10)\n",
    "inputs = torch.tensor(data_np)       # shape: (1200, 784)\n",
    "labels = torch.tensor(labels_np)       # shape: (1200, 10)\n",
    "\n",
    "# --- Initialize weights ---\n",
    "# In the C++/Eigen code, weights1 is created as a 784 x 256 matrix (column-major).\n",
    "# In PyTorch, we define weights1 as (784,256) so that when we do inputs.matmul(weights1)\n",
    "# with inputs of shape (1200,784), we get (1200,256). Similarly weights2 is (256,10).\n",
    "# Here we initialize them with small random values.\n",
    "weights1 = (torch.rand(784, 256) * 0.01).requires_grad_(True)\n",
    "weights2 = (torch.rand(256, 10) * 0.01).requires_grad_(True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(10):\n",
    "    # Forward pass\n",
    "    # In the original C++ code, inputs is transposed to become (1200,784).\n",
    "    # Since our PyTorch inputs are already in this shape, we can use them directly.\n",
    "    a1 = inputs.matmul(weights1)   # (1200,784) @ (784,256) => (1200,256)\n",
    "    z1 = a1.clamp(min=0)             # ReLU activation\n",
    "\n",
    "    a2 = z1.matmul(weights2)         # (1200,256) @ (256,10) => (1200,10)\n",
    "\n",
    "    # Manually compute the softmax (avoiding broadcasting pitfalls by explicitly summing over classes)\n",
    "    # Note: The denominator is computed per example (row).\n",
    "    exp_a2 = torch.exp(a2)\n",
    "    epsilon = 1e-10\n",
    "    denom = exp_a2.sum(dim=1, keepdim=True) + epsilon  # shape: (1200,1)\n",
    "    z2 = exp_a2 / denom                                  # shape: (1200,10), each row sums to 1\n",
    "\n",
    "    # Compute cross entropy loss manually.\n",
    "    # In the C++ code, the loss is computed as:\n",
    "    # loss = ((-1.0) * log(z2) * labels.transpose()).sum(0).sum(1)\n",
    "    # Because the original labels were {10,1200} and then transposed to (1200,10).\n",
    "    # Our labels are already (1200,10), so we compute:\n",
    "    loss = (-1.0 * torch.log(z2 + epsilon) * labels).sum() # scalar loss\n",
    "    loss /= inputs.shape[0]\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Backward pass: compute gradients with respect to weights.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using a manual gradient descent step.\n",
    "    with torch.no_grad():\n",
    "        weights1 -= learning_rate * weights1.grad\n",
    "        weights2 -= learning_rate * weights2.grad\n",
    "\n",
    "        # Zero the gradients after updating.\n",
    "        weights1.grad.zero_()\n",
    "        weights2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
