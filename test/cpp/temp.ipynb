{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "weights1 = torch.tensor([[0.1, 0.2],\n",
    "                        [0.3, 0.4]], dtype=torch.float32, requires_grad=True)\n",
    "inputs   = torch.tensor([[0.05, 0.15],\n",
    "                        [0.25, 0.35]], dtype=torch.float32, requires_grad=True)\n",
    "weights2 = torch.tensor([[0.01, 0.02],\n",
    "                        [0.03, 0.04]], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "layer1_linear = weights1.matmul(inputs.transpose(0,1))\n",
    "layer1_activated = torch.relu(layer1_linear)\n",
    "\n",
    "layer2_linear = layer1_activated.matmul(weights2)\n",
    "layer2_sum = layer2_linear.sum(1, keepdim=True)  # Keep dimension to make it [2,1] instead of [2]\n",
    "layer2_output = layer2_sum\n",
    "\n",
    "expand_tensor = torch.tensor([[1.0, 1.0]], dtype=torch.float32)\n",
    "layer2_output_expanded = layer2_output.matmul(expand_tensor)\n",
    "\n",
    "layer3_linear = layer2_output_expanded * weights1\n",
    "layer3_nonlinear = torch.exp(layer3_linear)\n",
    "layer3_output = layer3_nonlinear + layer2_output_expanded\n",
    "\n",
    "logits = layer3_output.matmul(weights2.transpose(0,1))\n",
    "\n",
    "probabilities = 1.0 / (1.0 + torch.exp(-1.0 * logits))\n",
    "\n",
    "epsilon = 1e-10\n",
    "log_probs = torch.log(probabilities + epsilon)\n",
    "neg_log_probs = -1.0 * log_probs + 0.1\n",
    "\n",
    "loss_sum = neg_log_probs.sum(0).sum(0)\n",
    "\n",
    "loss_sum.backward()\n",
    "\n",
    "print(\"Gradient of weights1:\")\n",
    "print(weights1.grad)\n",
    "print(\"\\nGradient of inputs:\")\n",
    "print(inputs.grad)\n",
    "print(\"\\nGradient of weights2:\")\n",
    "print(weights2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- Helper functions to simulate CSV reading and flattening ---\n",
    "\n",
    "def read_csv(file_path):\n",
    "    # For demonstration purposes, we simulate reading MNIST-like data.\n",
    "    # Let's assume we have 1200 examples, each with 784 features.\n",
    "    # And the labels are one-hot encoded vectors of length 10.\n",
    "    num_examples = 1200\n",
    "    num_features = 784\n",
    "    num_classes = 10\n",
    "\n",
    "    # Simulated input data: shape (1200, 784)\n",
    "    data = np.random.rand(num_examples, num_features).astype(np.float32)\n",
    "    # Simulated labels: create one-hot encoded labels\n",
    "    labels = np.zeros((num_examples, num_classes), dtype=np.float32)\n",
    "    random_class = np.random.randint(0, num_classes, size=num_examples)\n",
    "    labels[np.arange(num_examples), random_class] = 1.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def flatten(arr):\n",
    "    # In Python with numpy, flattening is straightforward\n",
    "    return arr.flatten()\n",
    "\n",
    "# --- Read data ---\n",
    "data_np, labels_np = read_csv(\"data/mnist_dummy.csv\")\n",
    "\n",
    "# In your C++ code the inputs tensor shape was {784, 1200} then transposed to (1200,784).\n",
    "# In PyTorch it's simpler: we make 'inputs' of shape (1200,784) and 'labels' of shape (1200,10).\n",
    "inputs = torch.tensor(data_np)       # shape: (1200, 784)\n",
    "labels = torch.tensor(labels_np)       # shape: (1200, 10)\n",
    "\n",
    "# --- Initialize weights ---\n",
    "# weights1 shape: (784, 256), weights2 shape: (256, 10)\n",
    "# They are initialized with small random numbers: scaled by 0.01.\n",
    "weights1 = (torch.rand(784, 256) * 0.01).requires_grad_(True)\n",
    "weights2 = (torch.rand(256, 10) * 0.01).requires_grad_(True)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(10):\n",
    "    # Forward pass\n",
    "    # Equivalent to: \n",
    "    #    a1 = inputs.transpose().matmul(weights1) in the C++ code,\n",
    "    # here we use inputs (shape: 1200x784) directly:\n",
    "    a1 = inputs.matmul(weights1)   # (1200,784) @ (784,256) = (1200,256)\n",
    "    z1 = a1.clamp(min=0)             # ReLU activation\n",
    "\n",
    "    a2 = z1.matmul(weights2)         # (1200,256) @ (256,10) = (1200,10)\n",
    "\n",
    "    # Compute softmax manually:\n",
    "    exp_a2 = torch.exp(a2)\n",
    "    # The denominator: sum over classes for each example.\n",
    "    # Adding a small epsilon (1e-10) for numerical stability.\n",
    "    denom = exp_a2.sum(dim=1, keepdim=True) + 1e-10  # shape: (1200,1)\n",
    "    z2 = exp_a2 / denom            # Broadcasting to (1200,10)\n",
    "\n",
    "    # Compute the cross-entropy loss manually.\n",
    "    # In the original code, loss is computed as:\n",
    "    #   loss = ((-1.0) * log(z2) * labels.transpose()).sum(0).sum(1)\n",
    "    # In our case, labels are (1200, 10) so we can compute the elementwise product\n",
    "    loss = (-1.0 * torch.log(z2 + 1e-10) * labels).sum()  # Scalar loss\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Backward pass: compute gradients with respect to weights.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using a manual gradient descent step.\n",
    "    with torch.no_grad():\n",
    "        weights1 -= learning_rate * weights1.grad\n",
    "        weights2 -= learning_rate * weights2.grad\n",
    "\n",
    "        # Zero the gradients after updating\n",
    "        weights1.grad.zero_()\n",
    "        weights2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create two 2x2 tensors with gradients enabled.\n",
    "a = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0]], requires_grad=True)\n",
    "b = torch.tensor([[0.1, 0.2],\n",
    "                  [0.3, 0.4]], requires_grad=True)\n",
    "\n",
    "# --- Test 1: Sum ---\n",
    "# Sum over dimension 0 (i.e. summing rows) and keep the dimension.\n",
    "sum_result = a.sum(dim=0, keepdim=True)  \n",
    "print(sum_result)\n",
    "# Because sum_result is not scalar (it's shape [1,2]),\n",
    "# we pass an explicit gradient tensor of matching shape.\n",
    "sum_result.backward(torch.ones_like(sum_result))\n",
    "print(\"Sum gradient:\")\n",
    "print(a.grad)\n",
    "# Reset a's gradients.\n",
    "a.grad.zero_()\n",
    "\n",
    "# --- Test 2: Matmul ---\n",
    "matmul_result = a.matmul(b)  # Matmul returns a [2,2] tensor.\n",
    "matmul_result.backward(torch.ones_like(matmul_result))\n",
    "print(\"\\nMatmul gradient for a:\")\n",
    "print(a.grad)\n",
    "print(\"\\nMatmul gradient for b:\")\n",
    "print(b.grad)\n",
    "a.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "# --- Test 3: Transpose ---\n",
    "# Compute the transpose of 'a'.\n",
    "transpose_result = a.transpose(0, 1)  # Still 2x2.\n",
    "transpose_result.backward(torch.ones_like(transpose_result))\n",
    "print(\"\\nTranspose gradient:\")\n",
    "print(a.grad)\n",
    "a.grad.zero_()\n",
    "\n",
    "# --- Test 4: Exponential ---\n",
    "exp_result = a.exp()  # Elementwise exponentiation.\n",
    "exp_result.backward(torch.ones_like(exp_result))\n",
    "print(\"\\nExp gradient:\")\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "weights1 = torch.tensor([[0.1, 0.3],\n",
    "                        [0.2, 0.4]], dtype=torch.float32, requires_grad=True)\n",
    "inputs   = torch.tensor([[0.05, 0.25],\n",
    "                        [0.15, 0.35]], dtype=torch.float32, requires_grad=True)\n",
    "weights2 = torch.tensor([[0.01, 0.03],\n",
    "                        [0.02, 0.04]], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "layer1_linear = weights1.matmul(inputs.transpose(0,1))\n",
    "layer1_activated = torch.relu(layer1_linear)\n",
    "\n",
    "layer2_linear = layer1_activated.matmul(weights2)\n",
    "layer2_sum = layer2_linear.sum(1, keepdim=True)  # Keep dimension to make it [2,1] instead of [2]\n",
    "layer2_output = layer2_sum\n",
    "\n",
    "expand_tensor = torch.tensor([[1.0, 1.0]], dtype=torch.float32)\n",
    "layer2_output_expanded = layer2_output.matmul(expand_tensor)\n",
    "\n",
    "layer3_linear = layer2_output_expanded * weights1\n",
    "layer3_nonlinear = torch.exp(layer3_linear)\n",
    "layer3_output = layer3_nonlinear + layer2_output_expanded\n",
    "\n",
    "logits = layer3_output.matmul(weights2.transpose(0,1))\n",
    "\n",
    "probabilities = 1.0 / (1.0 + torch.exp(-1.0 * logits))\n",
    "\n",
    "epsilon = 1e-10\n",
    "log_probs = torch.log(probabilities + epsilon)\n",
    "neg_log_probs = -1.0 * log_probs + 0.1\n",
    "\n",
    "loss_sum = neg_log_probs.sum(0).sum(0)\n",
    "\n",
    "loss_sum.backward()\n",
    "\n",
    "print(\"Gradient of weights1:\")\n",
    "print(weights1.grad)\n",
    "print(\"\\nGradient of inputs:\")\n",
    "print(inputs.grad)\n",
    "print(\"\\nGradient of weights2:\")\n",
    "print(weights2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "weights1 = torch.tensor([[0.1, 0.3],\n",
    "                        [0.2, 0.4]], dtype=torch.float32, requires_grad=True)\n",
    "inputs = torch.tensor([[0.05, 0.25],\n",
    "                      [0.15, 0.35]], dtype=torch.float32)\n",
    "weights2 = torch.tensor([[0.01, 0.03],\n",
    "                        [0.02, 0.04]], dtype=torch.float32)\n",
    "\n",
    "layer1 = weights1.matmul(inputs.transpose(0,1))\n",
    "layer1.retain_grad()\n",
    "layer1_activated = torch.relu(layer1)\n",
    "layer1_activated.retain_grad()\n",
    "\n",
    "# print(layer1_activated)\n",
    "# print(weights2)\n",
    "\n",
    "layer2 = layer1_activated.matmul(weights2) # HERE\n",
    "# print(layer2)\n",
    "layer2.retain_grad()\n",
    "\n",
    "layer2_sum = layer2.sum(1, keepdim=True)\n",
    "layer2_sum.retain_grad()\n",
    "layer2_expanded = layer2_sum.matmul(torch.tensor([[1.0, 1.0]], dtype=torch.float32))\n",
    "layer2_expanded.retain_grad()\n",
    "\n",
    "layer3 = layer2_expanded * weights1  # Element-wise multiplication\n",
    "layer3.retain_grad()\n",
    "loss = layer3.sum()\n",
    "\n",
    "\n",
    "print(\"of weights1:\")\n",
    "print(weights1)\n",
    "\n",
    "print(\"\\nof layer1:\")\n",
    "print(layer1)\n",
    "\n",
    "print(\"\\nof layer1_activated:\")\n",
    "print(layer1_activated)\n",
    "\n",
    "print(\"\\nof layer2:\")\n",
    "print(layer2)\n",
    "\n",
    "print(\"\\nof layer2_sum:\")\n",
    "print(layer2_sum)\n",
    "\n",
    "print(\"\\nof layer2_expanded:\")\n",
    "print(layer2_expanded)\n",
    "\n",
    "print(\"\\nof layer3:\")\n",
    "print(layer3)\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradient of weights1:\")\n",
    "print(weights1.grad)\n",
    "\n",
    "print(\"\\nGradient of layer1:\")\n",
    "print(layer1.grad)\n",
    "\n",
    "print(\"\\nGradient of layer1_activated:\")\n",
    "print(layer1_activated.grad)\n",
    "\n",
    "print(\"\\nGradient of layer2:\")\n",
    "print(layer2.grad)\n",
    "\n",
    "print(\"\\nGradient of layer2_sum:\")\n",
    "print(layer2_sum.grad)\n",
    "\n",
    "print(\"\\nGradient of layer2_expanded:\")\n",
    "print(layer2_expanded.grad)\n",
    "\n",
    "print(\"\\nGradient of layer3:\")\n",
    "print(layer3.grad)\n",
    "\n",
    "\n",
    "'''\n",
    "Gradient of weights1: \n",
    "Tensor(data=[0.0146, 0.0214, 0.0226, 0.0334], shape=[2, 2])\n",
    "\n",
    "Gradient of layer1: HERE!\n",
    "Tensor(data=[0.012, 0.018, 0.028, 0.042], shape=[2, 2])\n",
    "\n",
    "Gradient of layer1_activated:\n",
    "Tensor(data=[0.012, 0.018, 0.028, 0.042], shape=[2, 2])\n",
    "\n",
    "Gradient of layer2: HERE!\n",
    "Tensor(data=[0.4, 0.6, 0.4, 0.6], shape=[2, 2])\n",
    "\n",
    "Gradient of layer2_sum: \n",
    "Tensor(data=[0.4, 0.6], shape=[2, 1])\n",
    "\n",
    "Gradient of layer2_expanded:\n",
    "Tensor(data=[0.1, 0.2, 0.3, 0.4], shape=[2, 2])\n",
    "\n",
    "Gradient of layer3:\n",
    "Tensor(data=[1, 1, 1, 1], shape=[2, 2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# weights2 = torch.tensor([[0.01, 0.02], [0.03, 0.04]], requires_grad=True)\n",
    "# inputs = torch.tensor([[0.05, 0.15], [0.25, 0.35]], requires_grad=True)\n",
    "# weights1 = torch.tensor([[0.1, 0.2], [0.3, 0.4]], requires_grad=True)\n",
    "\n",
    "# weights1 = torch.tensor([[0.1, 0.3], [0.2, 0.4]], requires_grad=True)\n",
    "# inputs = torch.tensor([[0.05, 0.25], [0.15, 0.35]], requires_grad=True)\n",
    "# weights2 = torch.tensor([[0.01, 0.03], [0.02, 0.04]], requires_grad=True)\n",
    "\n",
    "# print(weights1)\n",
    "# print(inputs.T)\n",
    "# layer1 = weights1.matmul(inputs.T)\n",
    "# print(layer1)\n",
    "# print(\"---\")\n",
    "\n",
    "# print(layer1)\n",
    "# print(layer2)\n",
    "# print(layer1.matmul(weights2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEIGHTS1: Variable(requires_grad=1, data=Tensor(data=[0.1, 0.2, 0.3, 0.4], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0)\n",
    "# INPUT: Variable(requires_grad=1, data=Tensor(data=[0.05, 0.25, 0.15, 0.35], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0x565f3ddb9c60)\n",
    "# LAYER1: Variable(requires_grad=1, data=Tensor(data=[0.035, 0.075, 0.095, 0.215], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0x565f3ddb9e10)\n",
    "# ---\n",
    "# LAYER1: Variable(requires_grad=1, data=Tensor(data=[0.035, 0.075, 0.095, 0.215], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0x565f3ddb9e10)\n",
    "# WEIGHTS2: Variable(requires_grad=1, data=Tensor(data=[0.01, 0.02, 0.03, 0.04], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0)\n",
    "# LAYER2: Variable(requires_grad=1, data=Tensor(data=[0.0026, 0.0074, 0.0037, 0.0105], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0x565f3ddb9fc0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
