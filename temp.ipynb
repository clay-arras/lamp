{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "weights1 = torch.tensor([[0.1, 0.2],\n",
    "                        [0.3, 0.4]], dtype=torch.float32, requires_grad=True)\n",
    "inputs   = torch.tensor([[0.05, 0.15],\n",
    "                        [0.25, 0.35]], dtype=torch.float32, requires_grad=True)\n",
    "weights2 = torch.tensor([[0.01, 0.02],\n",
    "                        [0.03, 0.04]], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "layer1_linear = weights1.matmul(inputs.transpose(0,1))\n",
    "layer1_activated = torch.relu(layer1_linear)\n",
    "\n",
    "layer2_linear = layer1_activated.matmul(weights2)\n",
    "layer2_sum = layer2_linear.sum(1, keepdim=True)  # Keep dimension to make it [2,1] instead of [2]\n",
    "layer2_output = layer2_sum\n",
    "\n",
    "expand_tensor = torch.tensor([[1.0, 1.0]], dtype=torch.float32)\n",
    "layer2_output_expanded = layer2_output.matmul(expand_tensor)\n",
    "\n",
    "layer3_linear = layer2_output_expanded * weights1\n",
    "layer3_nonlinear = torch.exp(layer3_linear)\n",
    "layer3_output = layer3_nonlinear + layer2_output_expanded\n",
    "\n",
    "logits = layer3_output.matmul(weights2.transpose(0,1))\n",
    "\n",
    "probabilities = 1.0 / (1.0 + torch.exp(-1.0 * logits))\n",
    "\n",
    "epsilon = 1e-10\n",
    "log_probs = torch.log(probabilities + epsilon)\n",
    "neg_log_probs = -1.0 * log_probs + 0.1\n",
    "\n",
    "loss_sum = neg_log_probs.sum(0).sum(0)\n",
    "\n",
    "loss_sum.backward()\n",
    "\n",
    "print(\"Gradient of weights1:\")\n",
    "print(weights1.grad)\n",
    "print(\"\\nGradient of inputs:\")\n",
    "print(inputs.grad)\n",
    "print(\"\\nGradient of weights2:\")\n",
    "print(weights2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create two 2x2 tensors with gradients enabled.\n",
    "a = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0]], requires_grad=True)\n",
    "b = torch.tensor([[0.1, 0.2],\n",
    "                  [0.3, 0.4]], requires_grad=True)\n",
    "\n",
    "# --- Test 1: Sum ---\n",
    "# Sum over dimension 0 (i.e. summing rows) and keep the dimension.\n",
    "sum_result = a.sum(dim=0, keepdim=True)  \n",
    "print(sum_result)\n",
    "# Because sum_result is not scalar (it's shape [1,2]),\n",
    "# we pass an explicit gradient tensor of matching shape.\n",
    "sum_result.backward(torch.ones_like(sum_result))\n",
    "print(\"Sum gradient:\")\n",
    "print(a.grad)\n",
    "# Reset a's gradients.\n",
    "a.grad.zero_()\n",
    "\n",
    "# --- Test 2: Matmul ---\n",
    "matmul_result = a.matmul(b)  # Matmul returns a [2,2] tensor.\n",
    "matmul_result.backward(torch.ones_like(matmul_result))\n",
    "print(\"\\nMatmul gradient for a:\")\n",
    "print(a.grad)\n",
    "print(\"\\nMatmul gradient for b:\")\n",
    "print(b.grad)\n",
    "a.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "# --- Test 3: Transpose ---\n",
    "# Compute the transpose of 'a'.\n",
    "transpose_result = a.transpose(0, 1)  # Still 2x2.\n",
    "transpose_result.backward(torch.ones_like(transpose_result))\n",
    "print(\"\\nTranspose gradient:\")\n",
    "print(a.grad)\n",
    "a.grad.zero_()\n",
    "\n",
    "# --- Test 4: Exponential ---\n",
    "exp_result = a.exp()  # Elementwise exponentiation.\n",
    "exp_result.backward(torch.ones_like(exp_result))\n",
    "print(\"\\nExp gradient:\")\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "weights1 = torch.tensor([[0.1, 0.3],\n",
    "                        [0.2, 0.4]], dtype=torch.float32, requires_grad=True)\n",
    "inputs   = torch.tensor([[0.05, 0.25],\n",
    "                        [0.15, 0.35]], dtype=torch.float32, requires_grad=True)\n",
    "weights2 = torch.tensor([[0.01, 0.03],\n",
    "                        [0.02, 0.04]], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "layer1_linear = weights1.matmul(inputs.transpose(0,1))\n",
    "layer1_activated = torch.relu(layer1_linear)\n",
    "\n",
    "layer2_linear = layer1_activated.matmul(weights2)\n",
    "layer2_sum = layer2_linear.sum(1, keepdim=True)  # Keep dimension to make it [2,1] instead of [2]\n",
    "layer2_output = layer2_sum\n",
    "\n",
    "expand_tensor = torch.tensor([[1.0, 1.0]], dtype=torch.float32)\n",
    "layer2_output_expanded = layer2_output.matmul(expand_tensor)\n",
    "\n",
    "layer3_linear = layer2_output_expanded * weights1\n",
    "layer3_nonlinear = torch.exp(layer3_linear)\n",
    "layer3_output = layer3_nonlinear + layer2_output_expanded\n",
    "\n",
    "logits = layer3_output.matmul(weights2.transpose(0,1))\n",
    "\n",
    "probabilities = 1.0 / (1.0 + torch.exp(-1.0 * logits))\n",
    "\n",
    "epsilon = 1e-10\n",
    "log_probs = torch.log(probabilities + epsilon)\n",
    "neg_log_probs = -1.0 * log_probs + 0.1\n",
    "\n",
    "loss_sum = neg_log_probs.sum(0).sum(0)\n",
    "\n",
    "loss_sum.backward()\n",
    "\n",
    "print(\"Gradient of weights1:\")\n",
    "print(weights1.grad)\n",
    "print(\"\\nGradient of inputs:\")\n",
    "print(inputs.grad)\n",
    "print(\"\\nGradient of weights2:\")\n",
    "print(weights2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "weights1 = torch.tensor([[0.1, 0.3],\n",
    "                        [0.2, 0.4]], dtype=torch.float32, requires_grad=True)\n",
    "inputs = torch.tensor([[0.05, 0.25],\n",
    "                      [0.15, 0.35]], dtype=torch.float32)\n",
    "weights2 = torch.tensor([[0.01, 0.03],\n",
    "                        [0.02, 0.04]], dtype=torch.float32)\n",
    "\n",
    "layer1 = weights1.matmul(inputs.transpose(0,1))\n",
    "layer1.retain_grad()\n",
    "layer1_activated = torch.relu(layer1)\n",
    "layer1_activated.retain_grad()\n",
    "\n",
    "# print(layer1_activated)\n",
    "# print(weights2)\n",
    "\n",
    "layer2 = layer1_activated.matmul(weights2) # HERE\n",
    "# print(layer2)\n",
    "layer2.retain_grad()\n",
    "\n",
    "layer2_sum = layer2.sum(1, keepdim=True)\n",
    "layer2_sum.retain_grad()\n",
    "layer2_expanded = layer2_sum.matmul(torch.tensor([[1.0, 1.0]], dtype=torch.float32))\n",
    "layer2_expanded.retain_grad()\n",
    "\n",
    "layer3 = layer2_expanded * weights1  # Element-wise multiplication\n",
    "layer3.retain_grad()\n",
    "loss = layer3.sum()\n",
    "\n",
    "\n",
    "print(\"of weights1:\")\n",
    "print(weights1)\n",
    "\n",
    "print(\"\\nof layer1:\")\n",
    "print(layer1)\n",
    "\n",
    "print(\"\\nof layer1_activated:\")\n",
    "print(layer1_activated)\n",
    "\n",
    "print(\"\\nof layer2:\")\n",
    "print(layer2)\n",
    "\n",
    "print(\"\\nof layer2_sum:\")\n",
    "print(layer2_sum)\n",
    "\n",
    "print(\"\\nof layer2_expanded:\")\n",
    "print(layer2_expanded)\n",
    "\n",
    "print(\"\\nof layer3:\")\n",
    "print(layer3)\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradient of weights1:\")\n",
    "print(weights1.grad)\n",
    "\n",
    "print(\"\\nGradient of layer1:\")\n",
    "print(layer1.grad)\n",
    "\n",
    "print(\"\\nGradient of layer1_activated:\")\n",
    "print(layer1_activated.grad)\n",
    "\n",
    "print(\"\\nGradient of layer2:\")\n",
    "print(layer2.grad)\n",
    "\n",
    "print(\"\\nGradient of layer2_sum:\")\n",
    "print(layer2_sum.grad)\n",
    "\n",
    "print(\"\\nGradient of layer2_expanded:\")\n",
    "print(layer2_expanded.grad)\n",
    "\n",
    "print(\"\\nGradient of layer3:\")\n",
    "print(layer3.grad)\n",
    "\n",
    "\n",
    "'''\n",
    "Gradient of weights1: \n",
    "Tensor(data=[0.0146, 0.0214, 0.0226, 0.0334], shape=[2, 2])\n",
    "\n",
    "Gradient of layer1: HERE!\n",
    "Tensor(data=[0.012, 0.018, 0.028, 0.042], shape=[2, 2])\n",
    "\n",
    "Gradient of layer1_activated:\n",
    "Tensor(data=[0.012, 0.018, 0.028, 0.042], shape=[2, 2])\n",
    "\n",
    "Gradient of layer2: HERE!\n",
    "Tensor(data=[0.4, 0.6, 0.4, 0.6], shape=[2, 2])\n",
    "\n",
    "Gradient of layer2_sum: \n",
    "Tensor(data=[0.4, 0.6], shape=[2, 1])\n",
    "\n",
    "Gradient of layer2_expanded:\n",
    "Tensor(data=[0.1, 0.2, 0.3, 0.4], shape=[2, 2])\n",
    "\n",
    "Gradient of layer3:\n",
    "Tensor(data=[1, 1, 1, 1], shape=[2, 2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# weights2 = torch.tensor([[0.01, 0.02], [0.03, 0.04]], requires_grad=True)\n",
    "# inputs = torch.tensor([[0.05, 0.15], [0.25, 0.35]], requires_grad=True)\n",
    "# weights1 = torch.tensor([[0.1, 0.2], [0.3, 0.4]], requires_grad=True)\n",
    "\n",
    "# weights1 = torch.tensor([[0.1, 0.3], [0.2, 0.4]], requires_grad=True)\n",
    "# inputs = torch.tensor([[0.05, 0.25], [0.15, 0.35]], requires_grad=True)\n",
    "# weights2 = torch.tensor([[0.01, 0.03], [0.02, 0.04]], requires_grad=True)\n",
    "\n",
    "# print(weights1)\n",
    "# print(inputs.T)\n",
    "# layer1 = weights1.matmul(inputs.T)\n",
    "# print(layer1)\n",
    "# print(\"---\")\n",
    "\n",
    "# print(layer1)\n",
    "# print(layer2)\n",
    "# print(layer1.matmul(weights2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEIGHTS1: Variable(requires_grad=1, data=Tensor(data=[0.1, 0.2, 0.3, 0.4], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0)\n",
    "# INPUT: Variable(requires_grad=1, data=Tensor(data=[0.05, 0.25, 0.15, 0.35], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0x565f3ddb9c60)\n",
    "# LAYER1: Variable(requires_grad=1, data=Tensor(data=[0.035, 0.075, 0.095, 0.215], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0x565f3ddb9e10)\n",
    "# ---\n",
    "# LAYER1: Variable(requires_grad=1, data=Tensor(data=[0.035, 0.075, 0.095, 0.215], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0x565f3ddb9e10)\n",
    "# WEIGHTS2: Variable(requires_grad=1, data=Tensor(data=[0.01, 0.02, 0.03, 0.04], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0)\n",
    "# LAYER2: Variable(requires_grad=1, data=Tensor(data=[0.0026, 0.0074, 0.0037, 0.0105], shape=[2, 2]), grad=Tensor(data=[0, 0, 0, 0], shape=[2, 2]), grad_fn=0x565f3ddb9fc0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
